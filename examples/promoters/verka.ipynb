{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/slavic/PycharmProjects/promoters16/fold_1.csv\n"
     ]
    }
   ],
   "source": [
    "from examples.promoters.data import Promoters\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "dataset = Promoters([\n",
    "        \"/home/slavic/PycharmProjects/promoters16/fold_1.csv\",\n",
    "        # \"/home/slavic/PycharmProjects/promoters16/fold_2.csv\",\n",
    "        # \"/home/slavic/PycharmProjects/promoters16/fold_3.csv\",\n",
    "        # \"/home/slavic/PycharmProjects/promoters16/fold_4.csv\",\n",
    "        # \"/home/slavic/PycharmProjects/promoters16/fold_5.csv\",\n",
    "])\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(dataset, [int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)])\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=32)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from examples.promoters.modules import DataType\n",
    "from data_filters.sliding_window import SlidingWindowFilterTuple\n",
    "\n",
    "rollout = 1000\n",
    "data_filter = SlidingWindowFilterTuple[DataType](rollout, padding=200, skip_fields={\"target\", \"length\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AIRI-Institute/gena-lm-bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at AIRI-Institute/gena-lm-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from memup.preproc import IncrementStep\n",
    "from examples.promoters.modules import MemUpMemoryImpl\n",
    "from common_modules.transformers import BertRecurrentTransformerWithTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from gena_lm.modeling_bert import BertModel, BertForSequenceClassification\n",
    "from memup.base import MemoryRollout\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('AIRI-Institute/gena-lm-bert-base')\n",
    "bert_model: BertModel = BertForSequenceClassification.from_pretrained('AIRI-Institute/gena-lm-bert-base').bert\n",
    "mem_transformer = BertRecurrentTransformerWithTokenizer(bert_model, tokenizer, 300, 4, 3, bert_model.config.hidden_size * 2).cuda()\n",
    "\n",
    "memup_iter = MemoryRollout[DataType](\n",
    "    steps=2,\n",
    "    memory=MemUpMemoryImpl(mem_transformer),\n",
    "    data_filter=data_filter,\n",
    "    info_update=[IncrementStep()]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from memup.loss import TS\n",
    "from memup.base import MemoryOut, State, DataCollectorReplace\n",
    "\n",
    "\n",
    "class DataCollectorTrain(DataCollectorReplace[DataType, Tensor]):\n",
    "    def apply(self, data: DataType, out: MemoryOut, state: State) -> Tensor:\n",
    "        return state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slavic/PycharmProjects/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 11\u001B[0m\n\u001B[1;32m      7\u001B[0m info \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m---> 11\u001B[0m         data_collector, state, info, done \u001B[38;5;241m=\u001B[39m \u001B[43mmemup_iter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDataType\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mT\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDataCollectorTrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m         s \u001B[38;5;241m=\u001B[39m data_collector\u001B[38;5;241m.\u001B[39mresult()\n\u001B[1;32m     13\u001B[0m         \u001B[38;5;28mprint\u001B[39m(s[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m~/PycharmProjects/filtered-transformer/memup/base.py:130\u001B[0m, in \u001B[0;36mMemoryRollout.forward\u001B[0;34m(self, data, state, info, collector, steps)\u001B[0m\n\u001B[1;32m    127\u001B[0m         info \u001B[38;5;241m=\u001B[39m update\u001B[38;5;241m.\u001B[39mforward(data, state, info)\n\u001B[1;32m    129\u001B[0m filtered_data, done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_filter(data, state, info)\n\u001B[0;32m--> 130\u001B[0m out, state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmemory\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfiltered_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    131\u001B[0m collector\u001B[38;5;241m.\u001B[39mappend(filtered_data, out, state)\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m done:\n",
      "File \u001B[0;32m~/PycharmProjects/filtered-transformer/examples/promoters/modules.py:17\u001B[0m, in \u001B[0;36mMemUpMemoryImpl.forward\u001B[0;34m(self, data, state)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, data: DataType, state: State) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Optional[Tensor], State]:\n\u001B[0;32m---> 17\u001B[0m     os \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmem_tr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, os\u001B[38;5;241m.\u001B[39mstate\n",
      "File \u001B[0;32m~/PycharmProjects/filtered-transformer/common_modules/transformers.py:141\u001B[0m, in \u001B[0;36mBertRecurrentTransformerWithTokenizer.forward\u001B[0;34m(self, text_seq, state)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, text_seq: List[\u001B[38;5;28mstr\u001B[39m], state: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RecurrentOutputWithContext:\n\u001B[1;32m    140\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer(text_seq, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_len, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 141\u001B[0m     res \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mpad_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    142\u001B[0m            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m: pad_sequence([torch\u001B[38;5;241m.\u001B[39mtensor(t) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m tokens[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m]], batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    143\u001B[0m                                           padding_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mcuda()}\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mforward(res, state)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for text, labels in train_loader:\n",
    "        print()\n",
    "\n",
    "        state = torch.zeros(labels.shape[0], 50, bert_model.config.hidden_size).cuda()\n",
    "        T = len(text[0])\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "\n",
    "        while not done:\n",
    "                data_collector, state, info, done = memup_iter.forward(DataType(text, labels, T), state, info, DataCollectorTrain())\n",
    "                s = data_collector.result()\n",
    "                print(s[-1].shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
