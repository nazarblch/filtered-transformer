{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 21:42:50.617602: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-27 21:42:51.090046: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-27 21:42:51.090085: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-27 21:42:51.090090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/slavic/PycharmProjects/promoters16/fold_1.csv\n",
      "/home/slavic/PycharmProjects/promoters16/fold_2.csv\n",
      "/home/slavic/PycharmProjects/promoters16/fold_3.csv\n",
      "/home/slavic/PycharmProjects/promoters16/fold_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AIRI-Institute/gena-lm-bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at AIRI-Institute/gena-lm-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from omegaconf import OmegaConf\n",
    "from torch import nn\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from data_filters.sliding_window import SlidingWindowFilterTuple\n",
    "from examples.promoters.data import Promoters\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from metrics.f1 import F1Metric\n",
    "from memup.accumulator import Accumulator\n",
    "from memup.loss import PredictorLoss, LossModule, PredictorLossStateOnly, EvalLossStateOnly\n",
    "from memup.preproc import IncrementStep\n",
    "from examples.promoters.modules import MemUpMemoryImpl, DataCollectorTrain, DataCollectorLastState\n",
    "from common_modules.transformers import BertRecurrentTransformerWithTokenizer, BertClassifier\n",
    "from transformers import AutoTokenizer\n",
    "from gena_lm.modeling_bert import BertModel, BertForSequenceClassification\n",
    "from memup.base import MemoryRollout\n",
    "from examples.promoters.modules import DataType\n",
    "from metrics.accuracy import AccuracyMetric\n",
    "from absl import flags\n",
    "\n",
    "conf = OmegaConf.load('/home/slavic/PycharmProjects/filtered-transformer/examples/promoters/config.yaml')\n",
    "\n",
    "\n",
    "train_data = Promoters([os.path.join(conf.data.path, f) for f in conf.data.train])\n",
    "test_data = Promoters([os.path.join(conf.data.path, conf.data.test)])\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=conf.model.batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=conf.model.eval_batch_size)\n",
    "\n",
    "rollout = conf.model.rec_block_size\n",
    "state_length = conf.model.state_size\n",
    "data_filter = SlidingWindowFilterTuple[DataType](rollout, pad_fields={\"text\"}, padding=conf.model.rec_block_padding, skip_fields={\"target\", \"length\"})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('AIRI-Institute/gena-lm-bert-base')\n",
    "bert_model: BertModel = BertForSequenceClassification.from_pretrained('AIRI-Institute/gena-lm-bert-base').bert\n",
    "mem_transformer = BertRecurrentTransformerWithTokenizer(bert_model, tokenizer, conf.model.max_token_length, 4, 3, bert_model.config.hidden_size * 2).cuda()\n",
    "predictor = BertClassifier(2, bert_model.config, 4, 2, bert_model.config.hidden_size).cuda()\n",
    "\n",
    "weights = torch.load(\"/home/slavic/PycharmProjects/promoter.pt\")\n",
    "mem_transformer.load_state_dict(weights[\"mem\"])\n",
    "predictor.load_state_dict(weights[\"pred\"])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "fa_data = pd.read_csv(\"/home/slavic/Downloads/Telegram Desktop/pr_spl_fwd_test_mod.fa\", header=None)\n",
    "fa_data_list = [fa_data.iloc[i][0][:2000] for i in range(fa_data.shape[0])]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T = 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slavic/PycharmProjects/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5012,  1.9648],\n",
      "        [-0.2476,  0.0933],\n",
      "        [-1.6873,  1.2889],\n",
      "        [-1.3233,  0.8667],\n",
      "        [-1.3856,  0.9312],\n",
      "        [-0.1955,  0.0634],\n",
      "        [-0.8124,  0.5294],\n",
      "        [-2.4897,  1.9106],\n",
      "        [-2.4382,  1.9563],\n",
      "        [-1.6044,  1.0846]], device='cuda:0')\n",
      "[1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "mem_transformer.eval()\n",
    "predictor.eval()\n",
    "\n",
    "memup_iter = MemoryRollout[DataType](\n",
    "    steps=200,\n",
    "    memory=MemUpMemoryImpl(mem_transformer),\n",
    "    data_filter=data_filter,\n",
    "    info_update=[IncrementStep()]\n",
    ")\n",
    "\n",
    "\n",
    "predictor_loss = PredictorLossStateOnly(predictor, [\n",
    "        LossModule(nn.CrossEntropyLoss(), \"CE\", 1.0),\n",
    "        LossModule(AccuracyMetric(), \"Accuracy\", 0.0)\n",
    "])\n",
    "\n",
    "all_pred = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    text = fa_data_list\n",
    "    state2 = torch.zeros(len(text), state_length, bert_model.config.hidden_size, device=torch.device(\"cuda\"))\n",
    "    T = len(text[0])\n",
    "    print(\"T =\", T)\n",
    "\n",
    "    collector, _, _, _ = memup_iter.forward(DataType(text, labels, T), state2, {}, DataCollectorLastState())\n",
    "    target_seq, state_seq = collector.result()\n",
    "    s0 = torch.cat(state_seq, 0)\n",
    "    logits = predictor(s0)\n",
    "    pred = logits.argmax(-1).reshape(-1).cpu().numpy()\n",
    "    print(logits[:10])\n",
    "    print(pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
