{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/home/jovyan/filtered-transformer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset json (/home/jovyan/.cache/huggingface/datasets/json/default-b3b6f144a42f8309/0.0.0/fb88b12bd94767cb0cc7eedcd82ea1f402d2162addc03a37e81d4f8dc7313ad9)\n",
      "Using custom data configuration default\n",
      "Reusing dataset json (/home/jovyan/.cache/huggingface/datasets/json/default-ca7a0ff6209c9a75/0.0.0/fb88b12bd94767cb0cc7eedcd82ea1f402d2162addc03a37e81d4f8dc7313ad9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fda1a28a89545dea8d7444497bb1606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5ea6df61a643688a66bc9f04fcedc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8bf651da4248f2945bdbabcddde985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96da705b017340308f93efb9ee5117bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from examples.qa.data import get_tokenized_dataset\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "import transformers\n",
    "import tasks\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "from transformers.tokenization_utils_base import TruncationStrategy\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "\n",
    "def adjust_tokenizer(tokenizer):\n",
    "    if isinstance(tokenizer, (transformers.GPT2Tokenizer, transformers.GPT2TokenizerFast)) and \\\n",
    "            \"gpt\" in tokenizer.name_or_path:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    cache_dir=\"/home/jovyan/cashe\",\n",
    "    use_fast=True,\n",
    "    revision=\"main\",\n",
    ")\n",
    "\n",
    "adjust_tokenizer(tokenizer)\n",
    "\n",
    "\n",
    "task = tasks.get_task(task_args=tasks.TaskArguments(task_name=\"custom\", task_base_path=\"/home/jovyan/quality_mc/\"))\n",
    "dataset_dict = task.get_datasets()\n",
    "\n",
    "\n",
    "tokenized_dataset_dict = get_tokenized_dataset(\n",
    "    task=task,\n",
    "    dataset_dict=dataset_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=4096,\n",
    "    padding_strategy=PaddingStrategy(PaddingStrategy.MAX_LENGTH),\n",
    "    truncation_strategy=TruncationStrategy(TruncationStrategy.ONLY_FIRST),\n",
    "    model_mode=\"mc\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2523 test 2086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4.285730838775635 acc= 0.265625\n",
      "\n",
      "3.6775429248809814 acc= 0.40625\n",
      "\n",
      "4.105374813079834 acc= 0.3125\n",
      "\n",
      "3.9322423934936523 acc= 0.421875\n",
      "\n",
      "5.3573689460754395 acc= 0.328125\n",
      "\n",
      "3.4823191165924072 acc= 0.296875\n",
      "\n",
      "3.311509609222412 acc= 0.34375\n",
      "\n",
      "3.1081905364990234 acc= 0.390625\n",
      "\n",
      "2.629809856414795 acc= 0.453125\n",
      "\n",
      "4.760799884796143 acc= 0.34375\n",
      "\n",
      "4.753931522369385 acc= 0.1875\n",
      "\n",
      "3.519364833831787 acc= 0.25\n",
      "\n",
      "3.96626877784729 acc= 0.3125\n",
      "\n",
      "4.008843421936035 acc= 0.3125\n",
      "\n",
      "3.561490058898926 acc= 0.296875\n",
      "\n",
      "3.288618564605713 acc= 0.375\n",
      "\n",
      "3.7215704917907715 acc= 0.34375\n",
      "\n",
      "3.5406551361083984 acc= 0.375\n",
      "\n",
      "3.682309150695801 acc= 0.40625\n",
      "\n",
      "3.9537923336029053 acc= 0.34375\n",
      "\n",
      "3.9907119274139404 acc= 0.375\n",
      "\n",
      "3.1293444633483887 acc= 0.390625\n",
      "\n",
      "4.184239387512207 acc= 0.34375\n",
      "\n",
      "3.757068395614624 acc= 0.3125\n",
      "\n",
      "2.730419397354126 acc= 0.40625\n",
      "\n",
      "3.64552903175354 acc= 0.359375\n",
      "\n",
      "2.937997579574585 acc= 0.3125\n",
      "\n",
      "3.7049295902252197 acc= 0.328125\n",
      "\n",
      "3.7098772525787354 acc= 0.328125\n",
      "\n",
      "3.432802438735962 acc= 0.359375\n",
      "\n",
      "3.931098222732544 acc= 0.375\n",
      "\n",
      "2.9962501525878906 acc= 0.40625\n",
      "\n",
      "3.35227632522583 acc= 0.47368421052631576\n",
      "final acc 0.3480345158197507\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from metrics.accuracy import AccuracyMetric\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from examples.qa.modules import DataFilter, MemUpMemoryImpl, Predictor, RobertaRT\n",
    "from memup.base import DataCollectorAppend, DataCollectorReplace, MemoryRollout, State\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = tokenized_dataset_dict.get(\"train\")\n",
    "test_data = tokenized_dataset_dict.get(\"validation\")\n",
    "\n",
    "print(\"train\", len(train_data), \"test\", len(test_data))\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    batch_pt = {}\n",
    "        \n",
    "    for k in ['input_ids', 'attention_mask', \"label\", 'input_part_token_start_idx']:\n",
    "        batch_pt[k] = torch.stack(\n",
    "            [torch.tensor(el[k]) for el in batch]\n",
    "        )\n",
    "\n",
    "    return batch_pt\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(train_data, shuffle=False, batch_size=64, num_workers=8, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=64, num_workers=8, collate_fn=collate_fn)\n",
    "\n",
    "model = RobertaRT(RobertaModel.from_pretrained(\n",
    "    'roberta-base',\n",
    "    cache_dir=\"/home/jovyan/cashe\",\n",
    "    revision=\"main\",\n",
    ")).cuda()\n",
    "\n",
    "predictor = Predictor(model.bert.config).cuda()\n",
    "\n",
    "weights = torch.load(\"/home/jovyan/qa_1.1.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(weights[\"mem\"])\n",
    "predictor.load_state_dict(weights[\"pred\"])\n",
    "\n",
    "data_filter = DataFilter(tokenizer, 300)\n",
    "\n",
    "memup_iter = MemoryRollout[Dict[str, Tensor]](\n",
    "    steps=50,\n",
    "    memory=MemUpMemoryImpl(model),\n",
    "    data_filter=data_filter,\n",
    "    info_update=[]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class DataCollectorTrain(DataCollectorReplace[Dict[str, Tensor], Tensor]):\n",
    "    def apply(self, data: Dict[str, Tensor], out: Tensor, state: State) -> Tensor:\n",
    "        return state\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    all_pred = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "\n",
    "        labels = batch[\"label\"].cuda()\n",
    "        print()\n",
    "\n",
    "        state = torch.zeros(labels.shape[0] * 4, 50, 768, device=torch.device(\"cuda\"))\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        model.eval()\n",
    "        predictor.eval()\n",
    "\n",
    "        data_collector, state, info, done = memup_iter.forward(batch, state, info, DataCollectorTrain())\n",
    "        # opt.zero_grad()\n",
    "        states_seq = data_collector.result()\n",
    "        pred = predictor(states_seq[-1])\n",
    "        loss = nn.CrossEntropyLoss()(pred, labels)\n",
    "        acc = AccuracyMetric()(pred, labels)\n",
    "        \n",
    "        print(loss.item(), \"acc=\", acc)\n",
    "\n",
    "        all_pred.append(pred.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    acc = AccuracyMetric()(torch.cat(all_pred), torch.cat(all_labels))\n",
    "\n",
    "    print(\"final acc\", acc)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16, num_workers=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "from typing import Dict, Tuple\n",
    "from memup.base import Done, Info, State\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DataFilter(nn.Module):\n",
    "\n",
    "    def __init__(self, tokenizer, size) -> None:\n",
    "        super().__init__()\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.cls_token_id = tokenizer.cls_token_id\n",
    "        self.sep_token_id = tokenizer.sep_token_id\n",
    "        self.register_buffer('cls_token', torch.tensor([tokenizer.cls_token_id]))\n",
    "        self.register_buffer('sep_token', torch.tensor([tokenizer.sep_token_id]))\n",
    "        self.segment_size = size\n",
    "\n",
    "    def pad_add_special_tokens_for_qa(self, tensor, query_option):\n",
    "        input_elements = [self.cls_token, tensor, query_option]\n",
    "        tensor = torch.cat(input_elements)\n",
    "\n",
    "        pad_size = self.segment_size - tensor.shape[0]\n",
    "        if pad_size > 0:\n",
    "            tensor = F.pad(tensor, (0, pad_size), value=self.pad_token_id)\n",
    "        return tensor\n",
    "    \n",
    "    def get_attention_mask(self, tensor):\n",
    "        mask = torch.ones_like(tensor)\n",
    "        mask[tensor == self.pad_token_id] = 0\n",
    "        return mask\n",
    "\n",
    "    def get_token_type_ids(self, tensor):\n",
    "        return torch.zeros_like(tensor)\n",
    "\n",
    "    def get_cut_input(self, input_ids, input_part_token_start_idx, shift_batch):\n",
    "        # input_ids -> (b_s, 4, 4098)\n",
    "        B, _, T = input_ids.shape\n",
    "        input_ids = input_ids.reshape(B * 4, T)\n",
    "        input_part_token_start_idx = input_part_token_start_idx.reshape(B * 4)\n",
    "\n",
    "        end_seq = []\n",
    "        ss_batch = []\n",
    "    \n",
    "        for i, seq in enumerate(input_ids):\n",
    "            # seq = cls + context + sep + query + option + sep\n",
    "            spliter_inx = input_part_token_start_idx[i]\n",
    "            query_option = seq[spliter_inx:]\n",
    "            query_option = query_option[(query_option != self.pad_token_id) & (query_option != self.cls_token_id)]  # sep + query + option + sep\n",
    "            context = seq[:spliter_inx]  # cls + context\n",
    "            context = context[(context != self.pad_token_id) & (context != self.cls_token_id) & (context != self.sep_token_id)]  # context\n",
    "\n",
    "            start = shift_batch[i].item()\n",
    "            end = max(start, start + self.segment_size - len(query_option) - 1)\n",
    "            end = min(end, len(context))\n",
    "            # print(start, end)\n",
    "            input_segment = context[start:end] \n",
    "            input_segment = self.pad_add_special_tokens_for_qa(input_segment, query_option)\n",
    "            assert len(input_segment) == self.segment_size\n",
    "            ss_batch.append(input_segment)\n",
    "            end_seq.append(end)\n",
    "\n",
    "        # print(\"ends\", torch.stack(end_seq))\n",
    "        shift_batch = torch.tensor(end_seq)\n",
    "        ss_batch = torch.stack(ss_batch)\n",
    "\n",
    "        return ss_batch, shift_batch\n",
    "    \n",
    "    def forward(self, batch: Dict[str, Tensor], state: State = None, info: Info = {}, *args) -> Tuple[Dict[str, Tensor], Done]:\n",
    "\n",
    "        if \"shift_batch\" not in info:\n",
    "            info[\"shift_batch\"] = torch.zeros(batch['input_ids'].shape[0] * 4, dtype=torch.int32)\n",
    "        \n",
    "        input_ids, new_shift = self.get_cut_input(batch['input_ids'], batch['input_part_token_start_idx'], info[\"shift_batch\"])\n",
    "        done = (info[\"shift_batch\"] - new_shift).abs().sum().item() == 0\n",
    "        info[\"shift_batch\"] = new_shift\n",
    "\n",
    "        return {\n",
    "            \"label\": batch[\"label\"].cuda(),\n",
    "            'input_ids': input_ids.cuda(),\n",
    "            'attention_mask': self.get_attention_mask(input_ids).cuda(),\n",
    "            'token_type_ids': self.get_token_type_ids(input_ids).cuda()\n",
    "        }, done\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Predictor.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mexamples\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39madd_task\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m Predictor\n\u001b[1;32m      6\u001b[0m model \u001b[39m=\u001b[39m RobertaModel\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mroberta-base\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     cache_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/home/jovyan/cashe\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     revision\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> 12\u001b[0m mc_model \u001b[39m=\u001b[39m Predictor(model\u001b[39m.\u001b[39;49mconfig)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     14\u001b[0m data_filter \u001b[39m=\u001b[39m DataFilter(tokenizer, \u001b[39m512\u001b[39m)\n\u001b[1;32m     15\u001b[0m info \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mTypeError\u001b[0m: Predictor.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 512]) tensor([4059, 4065, 4063, 4063, 4075, 4075, 4075, 4075, 2589, 2589, 2589, 2589,\n",
      "        4065, 4061, 4070, 4070, 2962, 2962, 2962, 2962, 4059, 4057, 4058, 4058,\n",
      "        4051, 4068, 4061, 4065, 4053, 4054, 4054, 4057, 4065, 4067, 4067, 4065,\n",
      "        4054, 4055, 4054, 4052, 4060, 4063, 4061, 4066, 4078, 4079, 4077, 4080,\n",
      "        4064, 4070, 4061, 4066, 4078, 4076, 4079, 4079, 3962, 3962, 3962, 3962,\n",
      "        4064, 4067, 4062, 4059])\n"
     ]
    }
   ],
   "source": [
    "ss_batch, shift_batch = data_filter.get_cut_input(batch[\"input_ids\"], batch['input_part_token_start_idx'], shift_batch)\n",
    "print(ss_batch.shape, shift_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_config):\n",
    "        super().__init__()\n",
    "        config2 = deepcopy(bert_config)\n",
    "        config2.hidden_size = bert_config.hidden_size \n",
    "        config2.num_attention_heads = 4\n",
    "        config2.num_hidden_layers = 4\n",
    "\n",
    "        self.encoder = RobertaModel(config2).encoder\n",
    "        self.encoder.train()\n",
    "        self.config = config2\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.0),\n",
    "            nn.Linear(bert_config.hidden_size * 4, bert_config.hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(bert_config.hidden_size, 4),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        B, D = state.shape[0], state.shape[2]\n",
    "        out = self.encoder.forward(state)['last_hidden_state'][:, -1].reshape(B // 4, 768 * 4)\n",
    "        return self.head(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memup.base import MemUpMemory\n",
    "\n",
    "class RobertaRT(nn.Module):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, roberta: RobertaModel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = roberta\n",
    "        self.bert.train()\n",
    "\n",
    "        config2 = deepcopy(roberta.config)\n",
    "        config2.num_attention_heads = 4\n",
    "        config2.num_hidden_layers = 4\n",
    "\n",
    "        self.encoder = RobertaModel(config2).encoder\n",
    "        self.encoder.train()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        state,\n",
    "        input_ids: Tensor,\n",
    "        attention_mask: Tensor,\n",
    "        token_type_ids: Tensor\n",
    "    ):\n",
    "              \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        h = outputs[0]\n",
    "\n",
    "        hs = torch.cat([h, state], dim=1)\n",
    "        hs = self.encoder(hs)['last_hidden_state']\n",
    "        new_state = hs[:, h.shape[1]:]\n",
    "        # out = hs[:, : h.shape[1]]\n",
    "\n",
    "        empty_mask = attention_mask.type(torch.int32).sum(1)\n",
    "        new_state[empty_mask == 0] = state[empty_mask == 0]\n",
    "\n",
    "        return new_state\n",
    "    \n",
    "    \n",
    "class MemUpMemoryImpl(MemUpMemory):\n",
    "\n",
    "    def __init__(self, mem_tr: RobertaRT):\n",
    "        super().__init__()\n",
    "        self.mem_tr = mem_tr\n",
    "\n",
    "    def forward(self, data: Dict[str, Tensor], state: State) -> Tuple[Tensor, State]:\n",
    "        new_state = self.mem_tr.forward(state, data[\"input_ids\"], data[\"attention_mask\"], data[\"token_type_ids\"])\n",
    "        return None, new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4])\n",
      "0.375\n",
      "torch.Size([16, 4])\n",
      "0.25\n",
      "torch.Size([16, 4])\n",
      "0.3125\n",
      "torch.Size([16, 4])\n",
      "0.3125\n",
      "torch.Size([16, 4])\n",
      "0.4375\n",
      "torch.Size([16, 4])\n",
      "0.375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from memup.base import DataCollectorAppend, MemoryRollout\n",
    "from memup.loss import TS, LossModule, PredictorLossStateOnly\n",
    "from metrics.accuracy import AccuracyMetric\n",
    "\n",
    "\n",
    "model = RobertaRT(RobertaModel.from_pretrained(\n",
    "    'roberta-base',\n",
    "    cache_dir=\"/home/jovyan/cashe\",\n",
    "    revision=\"main\",\n",
    ")).cuda()\n",
    "\n",
    "predictor = Predictor(model.bert.config).cuda()\n",
    "\n",
    "data_filter = DataFilter(tokenizer, 512)\n",
    "\n",
    "memup_iter = MemoryRollout[Dict[str, Tensor]](\n",
    "    steps=2,\n",
    "    memory=MemUpMemoryImpl(model),\n",
    "    data_filter=data_filter,\n",
    "    info_update=[]\n",
    ")\n",
    "\n",
    "\n",
    "predictor_loss = PredictorLossStateOnly(predictor, [\n",
    "        LossModule(nn.CrossEntropyLoss(), \"CE\", 1.0),\n",
    "        LossModule(AccuracyMetric(), \"Accuracy\", 0.0)\n",
    "])\n",
    "\n",
    "class DataCollectorTrain(DataCollectorAppend[Dict[str, Tensor], Tensor]):\n",
    "    def apply(self, data: Dict[str, Tensor], out: Tensor, state: State) -> Tensor:\n",
    "        return state\n",
    "\n",
    "\n",
    "labels = batch[\"label\"].cuda()\n",
    "\n",
    "state = torch.zeros(labels.shape[0] * 4, 100, 768, device=torch.device(\"cuda\"))\n",
    "done = False\n",
    "info = {}\n",
    "\n",
    "model.train()\n",
    "predictor.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    while not done:\n",
    "        data_collector, state, info, done = memup_iter.forward(batch, state, info, DataCollectorTrain())\n",
    "        # opt.zero_grad()\n",
    "        states_seq = data_collector.result()\n",
    "        pred = predictor(states_seq[-1])\n",
    "        print(pred.shape)\n",
    "        loss = nn.CrossEntropyLoss()(pred, labels)\n",
    "        acc = AccuracyMetric()(pred, labels)\n",
    "        print(acc)\n",
    "\n",
    "        # loss.backward()\n",
    "        # opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
